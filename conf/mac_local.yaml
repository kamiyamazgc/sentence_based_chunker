runtime:
  device: "mps"
  batch_size: 64
  llm_concurrency: 2

llm:
  provider: "local"             # local / remote / auto
  local:
    model_path: "~/models/phi3-mini.Q4_K_M.gguf"
    server_url: "http://127.0.0.1:8000"
  remote:
    endpoint:  "https://api.openai.com/v1/chat/completions"
    model:     "gpt-4o-mini"

failover:
  f1_drop_threshold: 0.03       # 3pt 以上下落で警告ログ