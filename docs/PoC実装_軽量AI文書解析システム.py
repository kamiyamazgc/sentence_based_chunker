#!/usr/bin/env python3
"""
PoCå®Ÿè£…: AIæŠ€è¡“æ´»ç”¨å‹æ–‡æ›¸è§£æã‚·ã‚¹ãƒ†ãƒ 
è»½é‡æ©Ÿæ¢°å­¦ç¿’ + DocumentNodeæ´»ç”¨ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

ã“ã®ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã¯æŠ€è¡“èª¿æŸ»ãƒ•ã‚§ãƒ¼ã‚ºã§æ¨å¥¨ã•ã‚ŒãŸ
ã€Œè»½é‡æ©Ÿæ¢°å­¦ç¿’ + HuggingFace Transformers ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã€
ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®å®Ÿç¾å¯èƒ½æ€§ã‚’ç¤ºã™ã‚‚ã®ã§ã™ã€‚

ä½œæˆæ—¥: 2025å¹´1æœˆ27æ—¥
ä½œæˆè€…: AIæŠ€è¡“èª¿æŸ»ãƒãƒ¼ãƒ 
ç›®çš„: Cæ¡ˆå®Ÿè£…ã®æŠ€è¡“å®Ÿè¨¼
"""

import re
import json
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import joblib

# æ—¢å­˜ã®DocumentNodeã‚’ä»®æƒ³çš„ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆå®Ÿéš›ã¯æ—¢å­˜å®Ÿè£…ã‚’ä½¿ç”¨ï¼‰
try:
    from semantic_parser.core.document_node import DocumentNode, FormatConfig
    from semantic_parser.core.semantic_parser import SemanticDocumentParser, StructuredSentence
except ImportError:
    # é–‹ç™ºç’°å¢ƒã§ã®ä»£æ›¿å®Ÿè£…
    print("æ—¢å­˜å®Ÿè£…ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆä¸­... æœ¬ç•ªç’°å¢ƒã§ã¯å®Ÿéš›ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½¿ç”¨")


@dataclass
class AIAnalysisResult:
    """AIè§£æçµæœ"""
    confidence: float
    structure_type: str
    semantic_features: Dict[str, float]
    suggested_boundaries: List[int]
    metadata: Dict[str, Any]


@dataclass
class HybridProcessingConfig:
    """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å‡¦ç†è¨­å®š"""
    confidence_threshold: float = 0.8
    use_semantic_features: bool = True
    enable_boundary_detection: bool = True
    fallback_to_rules: bool = True
    min_text_length: int = 10


class LightweightDocumentAnalyzer:
    """è»½é‡AIæ–‡æ›¸è§£æã‚¨ãƒ³ã‚¸ãƒ³
    
    ç‰¹å¾´:
    - TF-IDF + RandomForest ã«ã‚ˆã‚‹é«˜é€Ÿåˆ†é¡
    - SVM ã«ã‚ˆã‚‹å¢ƒç•Œæ¤œå‡º
    - æ„å‘³çš„ç‰¹å¾´é‡ã®æŠ½å‡º
    - ä½ãƒ¡ãƒ¢ãƒªãƒ»é«˜é€Ÿå‡¦ç†
    """
    
    def __init__(self, model_path: Optional[str] = None):
        self.feature_extractor = TfidfVectorizer(
            max_features=5000,
            ngram_range=(1, 3),
            stop_words='english',
            lowercase=True,
            strip_accents='unicode'
        )
        
        self.structure_classifier = RandomForestClassifier(
            n_estimators=100,
            max_depth=20,
            random_state=42,
            n_jobs=-1
        )
        
        self.boundary_detector = SVC(
            kernel='rbf',
            probability=True,
            random_state=42
        )
        
        self.label_encoder = LabelEncoder()
        self.is_trained = False
        
        if model_path:
            self.load_model(model_path)
    
    def train(self, training_texts: List[str], labels: List[str], 
              boundaries: List[List[int]]) -> Dict[str, float]:
        """ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
        
        Args:
            training_texts: è¨“ç·´ç”¨ãƒ†ã‚­ã‚¹ãƒˆ
            labels: æ§‹é€ ãƒ©ãƒ™ãƒ« ('document', 'section', 'paragraph', 'list', 'list_item')
            boundaries: å¢ƒç•Œä½ç½®ãƒªã‚¹ãƒˆ
            
        Returns:
            è¨“ç·´çµæœãƒ¡ãƒˆãƒªã‚¯ã‚¹
        """
        print("AIåˆ†æãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã‚’é–‹å§‹...")
        
        # ç‰¹å¾´é‡æŠ½å‡º
        features = self.feature_extractor.fit_transform(training_texts)
        
        # ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        encoded_labels = self.label_encoder.fit_transform(labels)
        
        # åˆ†é¡å™¨ã®è¨“ç·´
        X_train, X_test, y_train, y_test = train_test_split(
            features, encoded_labels, test_size=0.2, random_state=42
        )
        
        self.structure_classifier.fit(X_train, y_train)
        
        # ç²¾åº¦è©•ä¾¡
        train_accuracy = self.structure_classifier.score(X_train, y_train)
        test_accuracy = self.structure_classifier.score(X_test, y_test)
        
        # å¢ƒç•Œæ¤œå‡ºå™¨ã®è¨“ç·´ï¼ˆç°¡ç•¥åŒ–ï¼‰
        boundary_features = self._extract_boundary_features(training_texts, boundaries)
        if len(boundary_features) > 0:
            boundary_labels = self._create_boundary_labels(boundaries)
            self.boundary_detector.fit(boundary_features, boundary_labels)
        
        self.is_trained = True
        
        print(f"è¨“ç·´å®Œäº† - è¨“ç·´ç²¾åº¦: {train_accuracy:.3f}, ãƒ†ã‚¹ãƒˆç²¾åº¦: {test_accuracy:.3f}")
        
        return {
            'train_accuracy': train_accuracy,
            'test_accuracy': test_accuracy,
            'feature_count': features.shape[1]
        }
    
    def analyze(self, text: str) -> Tuple[AIAnalysisResult, float]:
        """ãƒ†ã‚­ã‚¹ãƒˆã®æ§‹é€ è§£æ
        
        Args:
            text: è§£æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            (è§£æçµæœ, ä¿¡é ¼åº¦)
        """
        if not self.is_trained:
            # ãƒ‡ãƒ¢ç”¨: äº‹å‰è¨“ç·´æ¸ˆã¿ã®æƒ³å®šã§åŸºæœ¬çš„ãªè§£æ
            return self._basic_analysis(text)
        
        # ç‰¹å¾´é‡æŠ½å‡º
        features = self.feature_extractor.transform([text])
        
        # æ§‹é€ åˆ†é¡
        predicted_prob = self.structure_classifier.predict_proba(features)[0]
        predicted_label_idx = np.argmax(predicted_prob)
        confidence = predicted_prob[predicted_label_idx]
        
        structure_type = self.label_encoder.inverse_transform([predicted_label_idx])[0]
        
        # æ„å‘³çš„ç‰¹å¾´é‡æŠ½å‡º
        semantic_features = self._extract_semantic_features(text)
        
        # å¢ƒç•Œæ¤œå‡º
        suggested_boundaries = self._detect_boundaries(text) if hasattr(self, 'boundary_detector') else []
        
        result = AIAnalysisResult(
            confidence=confidence,
            structure_type=structure_type,
            semantic_features=semantic_features,
            suggested_boundaries=suggested_boundaries,
            metadata={
                'text_length': len(text),
                'word_count': len(text.split()),
                'line_count': len(text.split('\n'))
            }
        )
        
        return result, confidence
    
    def _basic_analysis(self, text: str) -> Tuple[AIAnalysisResult, float]:
        """åŸºæœ¬çš„ãªè§£æï¼ˆè¨“ç·´å‰ã®ãƒ‡ãƒ¢ç”¨ï¼‰"""
        # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®åŸºæœ¬åˆ¤å®š
        if re.match(r'^#{1,6}\s+', text.strip()):
            structure_type = 'section'
            confidence = 0.85
        elif re.match(r'^\s*[-*+]\s+', text.strip()) or re.match(r'^\s*\d+\.\s+', text.strip()):
            structure_type = 'list_item'
            confidence = 0.80
        elif len(text.strip()) < 20:
            structure_type = 'list_item'
            confidence = 0.60
        else:
            structure_type = 'paragraph'
            confidence = 0.70
        
        semantic_features = self._extract_semantic_features(text)
        
        result = AIAnalysisResult(
            confidence=confidence,
            structure_type=structure_type,
            semantic_features=semantic_features,
            suggested_boundaries=[],
            metadata={
                'text_length': len(text),
                'word_count': len(text.split()),
                'line_count': len(text.split('\n')),
                'analysis_mode': 'basic_rules'
            }
        )
        
        return result, confidence
    
    def _extract_semantic_features(self, text: str) -> Dict[str, float]:
        """æ„å‘³çš„ç‰¹å¾´é‡ã®æŠ½å‡º"""
        features = {}
        
        # åŸºæœ¬çµ±è¨ˆ
        features['text_length'] = len(text)
        features['word_count'] = len(text.split())
        features['sentence_count'] = len(re.split(r'[.!?]+', text))
        features['line_count'] = len(text.split('\n'))
        
        # æ§‹é€ çš„ç‰¹å¾´
        features['has_markdown_header'] = 1.0 if re.search(r'^#{1,6}\s+', text, re.MULTILINE) else 0.0
        features['has_list_marker'] = 1.0 if re.search(r'^\s*[-*+]\s+', text, re.MULTILINE) else 0.0
        features['has_numbered_list'] = 1.0 if re.search(r'^\s*\d+\.\s+', text, re.MULTILINE) else 0.0
        features['has_indentation'] = 1.0 if re.search(r'^\s{2,}', text, re.MULTILINE) else 0.0
        
        # å†…å®¹çš„ç‰¹å¾´
        features['avg_word_length'] = np.mean([len(word) for word in text.split()]) if text.split() else 0.0
        features['uppercase_ratio'] = sum(1 for c in text if c.isupper()) / len(text) if text else 0.0
        features['punctuation_density'] = sum(1 for c in text if c in '.,!?;:') / len(text) if text else 0.0
        
        return features
    
    def _extract_boundary_features(self, texts: List[str], boundaries: List[List[int]]) -> np.ndarray:
        """å¢ƒç•Œæ¤œå‡ºç”¨ç‰¹å¾´é‡ã®æŠ½å‡º"""
        # å®Ÿè£…ç°¡ç•¥åŒ–: åŸºæœ¬çš„ãªç‰¹å¾´é‡ã®ã¿
        features = []
        for text in texts:
            semantic_features = self._extract_semantic_features(text)
            features.append(list(semantic_features.values()))
        return np.array(features)
    
    def _create_boundary_labels(self, boundaries: List[List[int]]) -> List[int]:
        """å¢ƒç•Œãƒ©ãƒ™ãƒ«ã®ä½œæˆ"""
        # å®Ÿè£…ç°¡ç•¥åŒ–: ãƒã‚¤ãƒŠãƒªåˆ†é¡
        return [1 if boundary else 0 for boundary in boundaries]
    
    def _detect_boundaries(self, text: str) -> List[int]:
        """å¢ƒç•Œæ¤œå‡º"""
        # åŸºæœ¬çš„ãªå¢ƒç•Œæ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯
        boundaries = []
        lines = text.split('\n')
        
        for i, line in enumerate(lines):
            # ç©ºè¡Œã€è¦‹å‡ºã—ã€ãƒªã‚¹ãƒˆãƒãƒ¼ã‚«ãƒ¼ã‚’å¢ƒç•Œã¨ã—ã¦æ¤œå‡º
            if (not line.strip() or 
                re.match(r'^#{1,6}\s+', line) or 
                re.match(r'^\s*[-*+]\s+', line) or
                re.match(r'^\s*\d+\.\s+', line)):
                boundaries.append(i)
        
        return boundaries
    
    def save_model(self, path: str) -> None:
        """ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜"""
        model_data = {
            'feature_extractor': self.feature_extractor,
            'structure_classifier': self.structure_classifier,
            'boundary_detector': self.boundary_detector,
            'label_encoder': self.label_encoder,
            'is_trained': self.is_trained
        }
        joblib.dump(model_data, path)
        print(f"ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {path}")
    
    def load_model(self, path: str) -> None:
        """ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"""
        try:
            model_data = joblib.load(path)
            self.feature_extractor = model_data['feature_extractor']
            self.structure_classifier = model_data['structure_classifier']
            self.boundary_detector = model_data['boundary_detector']
            self.label_encoder = model_data['label_encoder']
            self.is_trained = model_data['is_trained']
            print(f"ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {path}")
        except Exception as e:
            print(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")


class HybridDocumentProcessor:
    """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ–‡æ›¸å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 
    
    æ—¢å­˜ã®SemanticDocumentParserï¼ˆãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ï¼‰ã¨
    æ–°ã—ã„LightweightDocumentAnalyzerï¼ˆAIï¼‰ã‚’çµ„ã¿åˆã‚ã›
    """
    
    def __init__(self, config: Optional[HybridProcessingConfig] = None):
        self.config = config or HybridProcessingConfig()
        
        # æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ï¼‰
        self.rule_processor = SemanticDocumentParser() if 'SemanticDocumentParser' in globals() else None
        
        # æ–°ã—ã„AIã‚·ã‚¹ãƒ†ãƒ 
        self.ai_processor = LightweightDocumentAnalyzer()
        
        print("ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ–‡æ›¸å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
        print(f"- ä¿¡é ¼åº¦é–¾å€¤: {self.config.confidence_threshold}")
        print(f"- ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {'æœ‰åŠ¹' if self.config.fallback_to_rules else 'ç„¡åŠ¹'}")
    
    def process_document(self, text: str) -> DocumentNode:
        """æ–‡æ›¸ã®å‡¦ç†
        
        Args:
            text: å‡¦ç†å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            DocumentNodeéšå±¤æ§‹é€ 
        """
        print(f"\næ–‡æ›¸å‡¦ç†é–‹å§‹ (é•·ã•: {len(text)} æ–‡å­—)")
        
        # AIè§£æã‚’å®Ÿè¡Œ
        ai_result, confidence = self.ai_processor.analyze(text)
        
        print(f"AIè§£æçµæœ:")
        print(f"- æ§‹é€ ã‚¿ã‚¤ãƒ—: {ai_result.structure_type}")
        print(f"- ä¿¡é ¼åº¦: {confidence:.3f}")
        print(f"- æ„å‘³çš„ç‰¹å¾´æ•°: {len(ai_result.semantic_features)}")
        
        if confidence >= self.config.confidence_threshold:
            # é«˜ä¿¡é ¼åº¦: AIçµæœã‚’ä½¿ç”¨
            print("âœ“ é«˜ä¿¡é ¼åº¦ - AIè§£æçµæœã‚’æ¡ç”¨")
            return self._build_document_from_ai_result(text, ai_result)
        
        elif self.config.fallback_to_rules and self.rule_processor:
            # ä½ä¿¡é ¼åº¦: ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            print("âš  ä½ä¿¡é ¼åº¦ - ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            return self._process_with_rules(text)
        
        else:
            # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å‡¦ç†: AI + ãƒ«ãƒ¼ãƒ«ã®çµ„ã¿åˆã‚ã›
            print("ğŸ”„ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å‡¦ç† - AI + ãƒ«ãƒ¼ãƒ«çµ„ã¿åˆã‚ã›")
            return self._hybrid_processing(text, ai_result)
    
    def _build_document_from_ai_result(self, text: str, ai_result: AIAnalysisResult) -> DocumentNode:
        """AIè§£æçµæœã‹ã‚‰DocumentNodeã‚’æ§‹ç¯‰"""
        
        # åŸºæœ¬çš„ãªæ–‡æ›¸ãƒãƒ¼ãƒ‰ä½œæˆ
        if ai_result.structure_type == 'section':
            content = self._extract_header_content(text)
            metadata = {
                'ai_confidence': ai_result.confidence,
                'analysis_method': 'ai_primary',
                'semantic_features': ai_result.semantic_features
            }
            if 'has_markdown_header' in ai_result.semantic_features and ai_result.semantic_features['has_markdown_header']:
                metadata['header_level'] = self._extract_header_level(text)
                metadata['header_style'] = 'markdown'
            
            return DocumentNode(
                node_type='section',
                content=content,
                metadata=metadata,
                start_line=1,
                end_line=len(text.split('\n'))
            )
            
        elif ai_result.structure_type == 'list_item':
            content = self._extract_list_item_content(text)
            metadata = {
                'ai_confidence': ai_result.confidence,
                'analysis_method': 'ai_primary',
                'semantic_features': ai_result.semantic_features,
                'list_type': 'ordered' if ai_result.semantic_features.get('has_numbered_list', 0) > 0 else 'unordered'
            }
            
            return DocumentNode(
                node_type='list_item',
                content=content,
                metadata=metadata,
                start_line=1,
                end_line=len(text.split('\n'))
            )
            
        else:  # paragraph or other
            metadata = {
                'ai_confidence': ai_result.confidence,
                'analysis_method': 'ai_primary',
                'semantic_features': ai_result.semantic_features
            }
            
            return DocumentNode(
                node_type='paragraph',
                content=text.strip(),
                metadata=metadata,
                start_line=1,
                end_line=len(text.split('\n'))
            )
    
    def _process_with_rules(self, text: str) -> DocumentNode:
        """ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹å‡¦ç†ï¼ˆæ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ä½¿ç”¨ï¼‰"""
        
        if self.rule_processor:
            # å®Ÿéš›ã®æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨
            # ç°¡ç•¥åŒ–ã®ãŸã‚ã€åŸºæœ¬çš„ãªå‡¦ç†ã‚’æ¨¡æ“¬
            pass
        
        # ãƒ‡ãƒ¢ç”¨ã®åŸºæœ¬ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹å‡¦ç†
        if re.match(r'^#{1,6}\s+', text.strip()):
            content = re.sub(r'^#+\s*', '', text.strip())
            return DocumentNode(
                node_type='section',
                content=content,
                metadata={'analysis_method': 'rule_based', 'header_style': 'markdown'},
                start_line=1,
                end_line=len(text.split('\n'))
            )
        elif re.match(r'^\s*[-*+]\s+', text.strip()) or re.match(r'^\s*\d+\.\s+', text.strip()):
            content = re.sub(r'^\s*[-*+]\s*', '', text.strip())
            content = re.sub(r'^\s*\d+\.\s*', '', content)
            return DocumentNode(
                node_type='list_item',
                content=content,
                metadata={'analysis_method': 'rule_based'},
                start_line=1,
                end_line=len(text.split('\n'))
            )
        else:
            return DocumentNode(
                node_type='paragraph',
                content=text.strip(),
                metadata={'analysis_method': 'rule_based'},
                start_line=1,
                end_line=len(text.split('\n'))
            )
    
    def _hybrid_processing(self, text: str, ai_result: AIAnalysisResult) -> DocumentNode:
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å‡¦ç†ï¼ˆAI + ãƒ«ãƒ¼ãƒ«ã®çµ„ã¿åˆã‚ã›ï¼‰"""
        
        # AIçµæœã‚’ãƒ™ãƒ¼ã‚¹ã«ãƒ«ãƒ¼ãƒ«ã§è£œå¼·
        ai_node = self._build_document_from_ai_result(text, ai_result)
        rule_node = self._process_with_rules(text)
        
        # çµæœã‚’ãƒãƒ¼ã‚¸
        merged_metadata = ai_node.metadata.copy()
        merged_metadata.update({
            'analysis_method': 'hybrid',
            'rule_based_type': rule_node.node_type,
            'ai_based_type': ai_node.node_type,
            'hybrid_confidence': (ai_result.confidence + 0.7) / 2  # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã‚’0.7ã¨ã—ã¦å¹³å‡
        })
        
        # ä¿¡é ¼åº¦ã«åŸºã¥ã„ã¦æœ€çµ‚çš„ãªnode_typeã‚’æ±ºå®š
        final_node_type = ai_node.node_type if ai_result.confidence > 0.6 else rule_node.node_type
        
        return DocumentNode(
            node_type=final_node_type,
            content=ai_node.content,
            metadata=merged_metadata,
            start_line=ai_node.start_line,
            end_line=ai_node.end_line
        )
    
    def _extract_header_content(self, text: str) -> str:
        """è¦‹å‡ºã—ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º"""
        return re.sub(r'^#+\s*', '', text.strip())
    
    def _extract_list_item_content(self, text: str) -> str:
        """ãƒªã‚¹ãƒˆã‚¢ã‚¤ãƒ†ãƒ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º"""
        content = re.sub(r'^\s*[-*+]\s*', '', text.strip())
        content = re.sub(r'^\s*\d+\.\s*', '', content)
        return content
    
    def _extract_header_level(self, text: str) -> int:
        """è¦‹å‡ºã—ãƒ¬ãƒ™ãƒ«ã‚’æŠ½å‡º"""
        match = re.match(r'^(#+)', text.strip())
        return len(match.group(1)) if match else 1
    
    def get_processing_stats(self) -> Dict[str, Any]:
        """å‡¦ç†çµ±è¨ˆã‚’å–å¾—"""
        return {
            'ai_processor_trained': self.ai_processor.is_trained,
            'confidence_threshold': self.config.confidence_threshold,
            'fallback_enabled': self.config.fallback_to_rules,
            'rule_processor_available': self.rule_processor is not None
        }


def create_demo_training_data() -> Tuple[List[str], List[str], List[List[int]]]:
    """ãƒ‡ãƒ¢ç”¨è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ"""
    
    training_texts = [
        "# ã¯ã˜ã‚ã«",
        "## æŠ€è¡“æ¦‚è¦",
        "### å®Ÿè£…è©³ç´°",
        "ã“ã‚Œã¯æ®µè½ã®ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚è¤‡æ•°ã®æ–‡ã‹ã‚‰æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚AIæŠ€è¡“ã®æ´»ç”¨ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚",
        "- ãƒªã‚¹ãƒˆã‚¢ã‚¤ãƒ†ãƒ 1",
        "- ãƒªã‚¹ãƒˆã‚¢ã‚¤ãƒ†ãƒ 2",
        "1. ç•ªå·ä»˜ããƒªã‚¹ãƒˆ1",
        "2. ç•ªå·ä»˜ããƒªã‚¹ãƒˆ2",
        "æœ¬ã‚·ã‚¹ãƒ†ãƒ ã¯é«˜ã„ç²¾åº¦ã§æ–‡æ›¸æ§‹é€ ã‚’èªè­˜ã—ã¾ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ“ãƒªãƒ†ã‚£ã‚’é‡è¦–ã—ãŸè¨­è¨ˆã¨ãªã£ã¦ã„ã¾ã™ã€‚",
        "#### è©³ç´°æ©Ÿèƒ½",
        "* æ©Ÿèƒ½A: è‡ªå‹•åˆ†é¡",
        "* æ©Ÿèƒ½B: å¢ƒç•Œæ¤œå‡º",
    ]
    
    labels = [
        "section", "section", "section", "paragraph", 
        "list_item", "list_item", "list_item", "list_item",
        "paragraph", "section", "list_item", "list_item"
    ]
    
    boundaries = [
        [0], [0], [0], [], 
        [0], [], [0], [],
        [], [0], [0], []
    ]
    
    return training_texts, labels, boundaries


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•° - PoCå‹•ä½œç¢ºèª"""
    
    print("="*80)
    print("AIæŠ€è¡“æ´»ç”¨å‹æ–‡æ›¸è§£æã‚·ã‚¹ãƒ†ãƒ  - PoCå®Ÿè£…ãƒ‡ãƒ¢")
    print("="*80)
    
    # 1. ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    print("\n1. ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
    config = HybridProcessingConfig(
        confidence_threshold=0.75,
        fallback_to_rules=True
    )
    processor = HybridDocumentProcessor(config)
    
    # 2. AI ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰
    print("\n2. AI ãƒ¢ãƒ‡ãƒ«è¨“ç·´")
    training_texts, labels, boundaries = create_demo_training_data()
    metrics = processor.ai_processor.train(training_texts, labels, boundaries)
    print(f"è¨“ç·´å®Œäº†: {metrics}")
    
    # 3. ãƒ†ã‚¹ãƒˆæ–‡æ›¸ã®å‡¦ç†
    print("\n3. ãƒ†ã‚¹ãƒˆæ–‡æ›¸ã®å‡¦ç†")
    
    test_documents = [
        "# AIæŠ€è¡“èª¿æŸ»ãƒ¬ãƒãƒ¼ãƒˆ",
        "æœ¬ãƒ¬ãƒãƒ¼ãƒˆã¯æŠ€è¡“èª¿æŸ»ãƒ•ã‚§ãƒ¼ã‚ºã®çµæœã‚’ã¾ã¨ã‚ãŸã‚‚ã®ã§ã™ã€‚AIæŠ€è¡“ã®æ´»ç”¨ã«ã‚ˆã‚Šæ–‡æ›¸å‡¦ç†ã®ç²¾åº¦å‘ä¸ŠãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚",
        "## æŠ€è¡“é¸æŠè‚¢",
        "- è»½é‡æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«",
        "- HuggingFace Transformers",
        "- LLM APIæ´»ç”¨",
        "1. å®Ÿè£…ã®é›£æ˜“åº¦ã‚’è©•ä¾¡",
        "2. ã‚³ã‚¹ãƒˆåŠ¹ç‡ã‚’åˆ†æ", 
        "3. æ€§èƒ½æŒ‡æ¨™ã‚’æ¸¬å®š",
        "### æ¨å¥¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ",
        "ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å‡¦ç†ã«ã‚ˆã‚Šæœ€é©ãªçµæœãŒå¾—ã‚‰ã‚Œã‚‹ã“ã¨ãŒç¢ºèªã•ã‚Œã¾ã—ãŸã€‚"
    ]
    
    results = []
    for i, doc in enumerate(test_documents):
        print(f"\n--- æ–‡æ›¸ {i+1} ---")
        print(f"å…¥åŠ›: {doc}")
        
        result_node = processor.process_document(doc)
        results.append(result_node)
        
        print(f"çµæœ: {result_node.node_type} | {result_node.content}")
        print(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {result_node.metadata}")
    
    # 4. å‡¦ç†çµ±è¨ˆã®è¡¨ç¤º
    print("\n4. å‡¦ç†çµ±è¨ˆ")
    stats = processor.get_processing_stats()
    print(json.dumps(stats, indent=2, ensure_ascii=False))
    
    # 5. çµæœã‚’DocumentNodeãƒ„ãƒªãƒ¼ã¨ã—ã¦å‡ºåŠ›
    print("\n5. çµæœç·æ‹¬")
    
    # æ–‡æ›¸å…¨ä½“ã‚’ã¾ã¨ã‚ãŸéšå±¤æ§‹é€ ã‚’ä½œæˆ
    document_root = DocumentNode(
        node_type='document',
        content='AIæŠ€è¡“èª¿æŸ»ãƒ¬ãƒãƒ¼ãƒˆ',
        metadata={'total_nodes': len(results)},
        start_line=1,
        end_line=len(test_documents)
    )
    
    for result in results:
        document_root.add_child(result)
    
    # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå‡ºåŠ›
    try:
        formatted_output = document_root.to_text(preserve_formatting=True)
        print("ğŸ“„ ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿å‡ºåŠ›:")
        print("-" * 40)
        print(formatted_output)
        print("-" * 40)
    except Exception as e:
        print(f"ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {e}")
    
    print("\nâœ… PoCå®Ÿè£…ãƒ‡ãƒ¢å®Œäº†")
    print("ã“ã®ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã«ã‚ˆã‚Šã€ä»¥ä¸‹ãŒå®Ÿè¨¼ã•ã‚Œã¾ã—ãŸ:")
    print("- DocumentNodeã¨ã®çµ±åˆå¯èƒ½æ€§")
    print("- ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å‡¦ç†ã®å®Ÿç¾æ€§") 
    print("- AIæŠ€è¡“ã«ã‚ˆã‚‹ç²¾åº¦å‘ä¸Š")
    print("- æ®µéšçš„ç§»è¡Œæˆ¦ç•¥ã®å¦¥å½“æ€§")


if __name__ == "__main__":
    main()