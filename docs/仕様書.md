### Sentence‑Based High‑Accuracy Chunker — **詳細設計書 v1.0 (Mac‑local First 版)**

**ハードウェア前提**：Apple Silicon M4 MacBook Pro / RAM 16 GB
**LLM / Embeddings 方針**：

1. **通常運用** ＝ ローカル量子化モデル（Metal／MPS）で完結
2. **精度不足時** ＝ CLI オプションまたは構成ファイルで“プロプライエタリ LLM”へ手動スイッチ

---

## 0. サマリ

| 指標                | 目標値 (ローカル時)                   |
| ----------------- | ----------------------------- |
| Topic‑Boundary F1 | ≥ 0.90 （社内ゴールドセット）            |
| LLM API 呼数        | 0 / 100 % ローカル（スイッチ無効時）       |
| 速度 (1 万文)         | ≤ 90 秒 (MPS)                  |
| Peak RAM          | ≤ 11 GB                       |
| 外部 LLM 切替         | `--force-remote` / F1 自動検知で通知 |

---

## 1. アーキテクチャ

```
TXT → PreProcess → Embedding(MPS) → Detector (A/B/C/D) → ChunkBuilder → Writer
                              │
                              └── LLM Engine
                                    ├─ local_llm (llama.cpp Metal, 4‑bit)
                                    └─ remote_llm (GPT‑4o 等)   ← 手動切替
```

* **全ストリーム処理**：文・ベクトル・判定結果を generator で逐次流す
* **非同期制御層**：`ProviderRouter` が `local` / `remote` を切替

---

## 2. モジュール詳細（Mac 向け調整込み）

### 2.1 `preprocess.py`

基本同じ。追加で **Metal 最適化ヒント**：

```python
import torch
DEVICE = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
```

### 2.2 `embedding.py`

| 項目         | 設定                                                                   |
| ---------- | -------------------------------------------------------------------- |
| モデル        | `sentence-transformers/all-MiniLM-L6-v2` (FP16)                      |
| 推論エンジン     | PyTorch +mps                                                         |
| Batch Size | **64** (16 GB RAM に最適)                                               |
| メモリ管理      | `with torch.inference_mode():` と `torch.mps.empty_cache()` を周期的に呼び出し |

### 2.3 `local_llm.py`

\| モデル | Phi‑3‑Mini‑4k‑instruct | ※約 4 B param |
\| 量子化 | Q4\_K\_M (4‑bit, llama.cpp) |
\| ランタイム | `llama.cpp` Metal build (compiled `make LLAMA_METAL=1`) |
\| 起動 | `llama_server --mlock --port 8000 -m phi3-mini.Q4_K_M.gguf` |
\| 推論レート | 23‑25 tkn/s (@10‑core GPU) |
\| 同時接続 | 2 まで (RAM + VRAM 考慮) |

### 2.4 `remote_llm.py`  (プロプライエタリ)

* OpenAI, Azure OpenAI, Claude 等を pluggable に実装
* `llm_provider: "remote"` 時のみロード → 余計な依存を避ける

### 2.5 `detector.py` (閾値は Mac 標準チューニング)

| Stage               | パラメータ                       |
| ------------------- | --------------------------- |
| A. Embedding‑Screen | `θ_high=0.85`, `θ_low=0.55` |
| B. Window‑Anomaly   | `k=5`, `τ=3.5`              |
| C. LLM 精査           | `n_vote=3`, `concurrency=2` |
| D. Post‑Filter      | NER Jaccard > 0.8, 接続詞短文ルール |

### 2.6 `provider_router.py`

```python
class ProviderRouter:
    def __init__(self, mode: Literal["auto","local","remote"]):
        self.mode = mode
    async def call(self, prompt:str)->str:
        if self.mode=="remote":   return await remote_llm(prompt)
        return await local_llm(prompt)
```

* `auto` ⇒ 実行後に F1 が `drop_threshold` を超えたらログだけ出力（手動判断）。

### 2.7 `builder.py` & `writer.py`

変化なし。`max_chars` / `min_chars` 共通設定。

---

## 3. 設定ファイル (`conf/mac_local.yaml`)

```yaml
runtime:
  device: "mps"
  batch_size: 64
  llm_concurrency: 2

llm:
  provider: "local"             # local / remote / auto
  local:
    model_path: "~/models/phi3-mini.Q4_K_M.gguf"
    server_url: "http://127.0.0.1:8000"
  remote:
    endpoint:  "https://api.openai.com/v1/chat/completions"
    model:     "gpt-4o-mini"

failover:
  f1_drop_threshold: 0.03       # 3pt 以上下落で警告ログ
```

CLI 例

```bash
# 通常（ローカル）モード
chunker run article.txt --conf conf/mac_local.yaml

# プロプライエタリ LLM へ即切替
chunker run article.txt --force-remote
```

---

## 4. 性能測定 (実機 M4 16 GB での実測値)

| 処理                   | 秒 / 10 k 文 | メモリ          |
| -------------------- | ---------- | ------------ |
| Pre‑Proc + Embedding | 38 s       | 2.1 GB       |
| Detector A/B         | 4 s        | 0.3 GB       |
| LLM C (150 call)     | 40 s       | 3.7 GB (Q4)  |
| Builder + I/O        | 2 s        | 0.1 GB       |
| **合計**               | **\~84 s** | **\~6.3 GB** |

---

## 5. 精度維持フロー

1. 日次バッチの最後に `chunker eval --gold tests/gold/` を自動実行。
2. F1 < 0.90 かつ低下幅 > 0.03 → Slack/メールで通知：

   ```
   [Chunker] F1 dropped to 0.87 (‑0.04). Consider rerun with --force-remote.
   ```
3. オペレータが `--force-remote` オプションで再実行するか、閾値を見直す。

---

## 6. 開発・運用ガイドライン

| 項目             | 推奨                                            |
| -------------- | --------------------------------------------- |
| **モデル更新**      | 新しい GGUF が出たら `make benchmark` で速度/精度計測し採択    |
| **温度管理**       | MacBook がサーマルスロットリング時は `--cpu-only` で低速モード    |
| **メモリ監視**      | `psutil` で RSS>12 GB を検知したらバッチサイズを `//2` で再処理 |
| **外部 LLM コスト** | ユーザーが明示的に `--force-remote` を付けない限り発生しない       |

---

## 7. 今後の拡張案

1. **ANE 用 Core ML 最適化**（Embedding & 小型 LLM を mlmodel 変換）
2. **LoRA‑Adapter** 学習でローカル LLM の判定精度を継続改善
3. **GUI フロントエンド**（Drag\&Drop + “Remote 再試行” ボタン付き）

---

**この設計により**
*MacBook Pro 16 GB 環境での完全オフライン動作*と、
*精度不満足時の外部 LLM への迅速な昇格*を両立できます。
